{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c86b01f4",
   "metadata": {},
   "source": [
    "# Data Acquisition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34ae96a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"F:/BigDataA3/.cache/hf_cache\"\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b17790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from bigdata_a3_utils import get_cache_directory\n",
    "\n",
    "get_cache_directory(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38ba8265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdata_a3_utils import download_all_amazon_reviews, load_compressed_dataset\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b00c9b",
   "metadata": {},
   "source": [
    "## Downloaded Dataset using Option D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b850bd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GZ compression at level 6\n",
      "[INFO] Compression speed: Medium\n",
      "[INFO] Compression ratio: Medium\n",
      "[SKIP] raw_review_All_Beauty already exists\n",
      "[SKIP] raw_meta_All_Beauty already exists\n",
      "✅ Processing All_Beauty\n",
      "[SKIP] raw_review_Amazon_Fashion already exists\n",
      "[SKIP] raw_meta_Amazon_Fashion already exists\n",
      "✅ Processing Amazon_Fashion\n",
      "[SKIP] raw_review_Appliances already exists\n",
      "[SKIP] raw_meta_Appliances already exists\n",
      "✅ Processing Appliances\n",
      "[SKIP] raw_review_Arts_Crafts_and_Sewing already exists\n",
      "[SKIP] raw_meta_Arts_Crafts_and_Sewing already exists\n",
      "✅ Processing Arts_Crafts_and_Sewing\n",
      "[SKIP] raw_review_Automotive already exists\n",
      "[SKIP] raw_meta_Automotive already exists\n",
      "✅ Processing Automotive\n",
      "[SKIP] raw_review_Baby_Products already exists\n",
      "[SKIP] raw_meta_Baby_Products already exists\n",
      "✅ Processing Baby_Products\n",
      "[SKIP] raw_review_Beauty_and_Personal_Care already exists\n",
      "[SKIP] raw_meta_Beauty_and_Personal_Care already exists\n",
      "✅ Processing Beauty_and_Personal_Care\n",
      "[SKIP] raw_review_Books already exists\n",
      "[SKIP] raw_meta_Books already exists\n",
      "✅ Processing Books\n",
      "[SKIP] raw_review_CDs_and_Vinyl already exists\n",
      "[SKIP] raw_meta_CDs_and_Vinyl already exists\n",
      "✅ Processing CDs_and_Vinyl\n",
      "[SKIP] raw_review_Cell_Phones_and_Accessories already exists\n",
      "[SKIP] raw_meta_Cell_Phones_and_Accessories already exists\n",
      "✅ Processing Cell_Phones_and_Accessories\n",
      "[SKIP] raw_review_Clothing_Shoes_and_Jewelry already exists\n",
      "[SKIP] raw_meta_Clothing_Shoes_and_Jewelry already exists\n",
      "✅ Processing Clothing_Shoes_and_Jewelry\n",
      "[SKIP] raw_review_Digital_Music already exists\n",
      "[SKIP] raw_meta_Digital_Music already exists\n",
      "✅ Processing Digital_Music\n",
      "[SKIP] raw_review_Electronics already exists\n",
      "[SKIP] raw_meta_Electronics already exists\n",
      "✅ Processing Electronics\n",
      "[SKIP] raw_review_Gift_Cards already exists\n",
      "[SKIP] raw_meta_Gift_Cards already exists\n",
      "✅ Processing Gift_Cards\n",
      "[SKIP] raw_review_Grocery_and_Gourmet_Food already exists\n",
      "[SKIP] raw_meta_Grocery_and_Gourmet_Food already exists\n",
      "✅ Processing Grocery_and_Gourmet_Food\n",
      "[SKIP] raw_review_Handmade_Products already exists\n",
      "[SKIP] raw_meta_Handmade_Products already exists\n",
      "✅ Processing Handmade_Products\n",
      "[SKIP] raw_review_Health_and_Household already exists\n",
      "[SKIP] raw_meta_Health_and_Household already exists\n",
      "✅ Processing Health_and_Household\n",
      "[SKIP] raw_review_Health_and_Personal_Care already exists\n",
      "[SKIP] raw_meta_Health_and_Personal_Care already exists\n",
      "✅ Processing Health_and_Personal_Care\n",
      "⠇ Processing Home_and_Kitchen "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec6d171900a46099869ad848a9cebc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Home_and_Kitchen.jsonl:   6%|6         | 1.97G/31.4G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m base_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF:/BigDataA3/data_compressed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m base_path\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m download_all_amazon_reviews(\n\u001b[0;32m      5\u001b[0m     base_save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(base_path),\n\u001b[0;32m      6\u001b[0m     categories\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# All categories\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     compress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      8\u001b[0m     compression_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgz\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     compression_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[0;32m     10\u001b[0m )\n",
      "File \u001b[1;32mf:\\BigDataA3\\bigdata_a3_utils.py:244\u001b[0m, in \u001b[0;36mdownload_all_amazon_reviews\u001b[1;34m(base_save_path, categories, compress, compression_format, compression_level)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m yaspin(text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m spinner:\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;66;03m# review dataset\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m         review_result \u001b[38;5;241m=\u001b[39m process_dataset(\n\u001b[0;32m    245\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    246\u001b[0m             category,\n\u001b[0;32m    247\u001b[0m             base_save_path,\n\u001b[0;32m    248\u001b[0m             compress,\n\u001b[0;32m    249\u001b[0m             compression_format,\n\u001b[0;32m    250\u001b[0m             compression_level\n\u001b[0;32m    251\u001b[0m         )\n\u001b[0;32m    252\u001b[0m         spinner\u001b[38;5;241m.\u001b[39mwrite(review_result)\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;66;03m# meta dataset\u001b[39;00m\n",
      "File \u001b[1;32mf:\\BigDataA3\\bigdata_a3_utils.py:164\u001b[0m, in \u001b[0;36mprocess_dataset\u001b[1;34m(dataset_type, category, base_save_path, compress, compression_format, compression_level)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SKIP] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# download and save\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMcAuley-Lab/Amazon-Reviews-2023\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    167\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    168\u001b[0m )\n\u001b[0;32m    169\u001b[0m dataset_path\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    170\u001b[0m dataset\u001b[38;5;241m.\u001b[39msave_to_disk(\u001b[38;5;28mstr\u001b[39m(dataset_path))\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\datasets\\load.py:2084\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2081\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[0;32m   2083\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[1;32m-> 2084\u001b[0m builder_instance\u001b[38;5;241m.\u001b[39mdownload_and_prepare(\n\u001b[0;32m   2085\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m   2086\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m   2087\u001b[0m     verification_mode\u001b[38;5;241m=\u001b[39mverification_mode,\n\u001b[0;32m   2088\u001b[0m     num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[0;32m   2089\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   2090\u001b[0m )\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   2093\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2094\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[0;32m   2095\u001b[0m )\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\datasets\\builder.py:925\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    924\u001b[0m     prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[1;32m--> 925\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_and_prepare(\n\u001b[0;32m    926\u001b[0m     dl_manager\u001b[38;5;241m=\u001b[39mdl_manager,\n\u001b[0;32m    927\u001b[0m     verification_mode\u001b[38;5;241m=\u001b[39mverification_mode,\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_split_kwargs,\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[0;32m    930\u001b[0m )\n\u001b[0;32m    931\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\datasets\\builder.py:1649\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[0;32m   1648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verification_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs):\n\u001b[1;32m-> 1649\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_download_and_prepare(\n\u001b[0;32m   1650\u001b[0m         dl_manager,\n\u001b[0;32m   1651\u001b[0m         verification_mode,\n\u001b[0;32m   1652\u001b[0m         check_duplicate_keys\u001b[38;5;241m=\u001b[39mverification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS\n\u001b[0;32m   1653\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS,\n\u001b[0;32m   1654\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs,\n\u001b[0;32m   1655\u001b[0m     )\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\datasets\\builder.py:979\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m    977\u001b[0m split_dict \u001b[38;5;241m=\u001b[39m SplitDict(dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name)\n\u001b[0;32m    978\u001b[0m split_generators_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[1;32m--> 979\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_generators(dl_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msplit_generators_kwargs)\n\u001b[0;32m    981\u001b[0m \u001b[38;5;66;03m# Checksums verification\u001b[39;00m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS \u001b[38;5;129;01mand\u001b[39;00m dl_manager\u001b[38;5;241m.\u001b[39mrecord_checksums:\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\McAuley-Lab--Amazon-Reviews-2023\\16b76e0823d73bb8cff1e9c5e3e37dbc46ae3daee380417ae141f5e67d3ea8e8\\Amazon-Reviews-2023.py:509\u001b[0m, in \u001b[0;36mAmazonReview2023._split_generators\u001b[1;34m(self, dl_manager)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_split_generators\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager):\n\u001b[1;32m--> 509\u001b[0m     dl_dir \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownload_and_extract(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdata_dir)\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, BenchmarkAmazonReview2023Config):\n\u001b[0;32m    511\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    512\u001b[0m             datasets\u001b[38;5;241m.\u001b[39mSplitGenerator(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, gen_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath\u001b[39m\u001b[38;5;124m\"\u001b[39m: dl_dir[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]}),\n\u001b[0;32m    513\u001b[0m             datasets\u001b[38;5;241m.\u001b[39mSplitGenerator(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m, gen_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath\u001b[39m\u001b[38;5;124m\"\u001b[39m: dl_dir[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m]}),\n\u001b[0;32m    514\u001b[0m             datasets\u001b[38;5;241m.\u001b[39mSplitGenerator(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, gen_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath\u001b[39m\u001b[38;5;124m\"\u001b[39m: dl_dir[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]}),\n\u001b[0;32m    515\u001b[0m         ]\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\datasets\\download\\download_manager.py:326\u001b[0m, in \u001b[0;36mDownloadManager.download_and_extract\u001b[1;34m(self, url_or_urls)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_and_extract\u001b[39m(\u001b[38;5;28mself\u001b[39m, url_or_urls):\n\u001b[0;32m    311\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Download and extract given `url_or_urls`.\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \n\u001b[0;32m    313\u001b[0m \u001b[38;5;124;03m    Is roughly equivalent to:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;124;03m        extracted_path(s): `str`, extracted paths of given URL(s).\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload(url_or_urls))\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\datasets\\download\\download_manager.py:159\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[1;34m(self, url_or_urls)\u001b[0m\n\u001b[0;32m    157\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m stack_multiprocessing_download_progress_bars():\n\u001b[1;32m--> 159\u001b[0m     downloaded_path_or_paths \u001b[38;5;241m=\u001b[39m map_nested(\n\u001b[0;32m    160\u001b[0m         download_func,\n\u001b[0;32m    161\u001b[0m         url_or_urls,\n\u001b[0;32m    162\u001b[0m         map_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    163\u001b[0m         num_proc\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mnum_proc,\n\u001b[0;32m    164\u001b[0m         desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading data files\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    165\u001b[0m         batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    166\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    167\u001b[0m     )\n\u001b[0;32m    168\u001b[0m duration \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m    169\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration\u001b[38;5;241m.\u001b[39mtotal_seconds()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\datasets\\utils\\py_utils.py:486\u001b[0m, in \u001b[0;36mmap_nested\u001b[1;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[0;32m    485\u001b[0m     data_struct \u001b[38;5;241m=\u001b[39m [data_struct]\n\u001b[1;32m--> 486\u001b[0m mapped \u001b[38;5;241m=\u001b[39m function(data_struct)\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[0;32m    488\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m mapped[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\datasets\\download\\download_manager.py:220\u001b[0m, in \u001b[0;36mDownloadManager._download_batched\u001b[1;34m(self, url_or_filenames, download_config)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m thread_map(\n\u001b[0;32m    207\u001b[0m         download_func,\n\u001b[0;32m    208\u001b[0m         url_or_filenames,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m         tqdm_class\u001b[38;5;241m=\u001b[39mtqdm,\n\u001b[0;32m    217\u001b[0m     )\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 220\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_single(url_or_filename, download_config\u001b[38;5;241m=\u001b[39mdownload_config)\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m url_or_filename \u001b[38;5;129;01min\u001b[39;00m url_or_filenames\n\u001b[0;32m    222\u001b[0m     ]\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\datasets\\download\\download_manager.py:229\u001b[0m, in \u001b[0;36mDownloadManager._download_single\u001b[1;34m(self, url_or_filename, download_config)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_relative_path(url_or_filename):\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# append the relative path to the base_path\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     url_or_filename \u001b[38;5;241m=\u001b[39m url_or_path_join(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_path, url_or_filename)\n\u001b[1;32m--> 229\u001b[0m out \u001b[38;5;241m=\u001b[39m cached_path(url_or_filename, download_config\u001b[38;5;241m=\u001b[39mdownload_config)\n\u001b[0;32m    230\u001b[0m out \u001b[38;5;241m=\u001b[39m tracked_str(out)\n\u001b[0;32m    231\u001b[0m out\u001b[38;5;241m.\u001b[39mset_origin(url_or_filename)\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\datasets\\utils\\file_utils.py:189\u001b[0m, in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m resolved_path \u001b[38;5;241m=\u001b[39m huggingface_hub\u001b[38;5;241m.\u001b[39mHfFileSystem(\n\u001b[0;32m    180\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mHF_ENDPOINT, token\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mtoken\n\u001b[0;32m    181\u001b[0m )\u001b[38;5;241m.\u001b[39mresolve_path(url_or_filename)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m huggingface_hub\u001b[38;5;241m.\u001b[39mHfApi(\n\u001b[0;32m    184\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mHF_ENDPOINT,\n\u001b[0;32m    185\u001b[0m         token\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mtoken,\n\u001b[0;32m    186\u001b[0m         library_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    187\u001b[0m         library_version\u001b[38;5;241m=\u001b[39m__version__,\n\u001b[0;32m    188\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39mget_datasets_user_agent(download_config\u001b[38;5;241m.\u001b[39muser_agent),\n\u001b[1;32m--> 189\u001b[0m     )\u001b[38;5;241m.\u001b[39mhf_hub_download(\n\u001b[0;32m    190\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mresolved_path\u001b[38;5;241m.\u001b[39mrepo_id,\n\u001b[0;32m    191\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mresolved_path\u001b[38;5;241m.\u001b[39mrepo_type,\n\u001b[0;32m    192\u001b[0m         revision\u001b[38;5;241m=\u001b[39mresolved_path\u001b[38;5;241m.\u001b[39mrevision,\n\u001b[0;32m    193\u001b[0m         filename\u001b[38;5;241m=\u001b[39mresolved_path\u001b[38;5;241m.\u001b[39mpath_in_repo,\n\u001b[0;32m    194\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mforce_download,\n\u001b[0;32m    195\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mproxies,\n\u001b[0;32m    196\u001b[0m     )\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[0;32m    198\u001b[0m     huggingface_hub\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mRepositoryNotFoundError,\n\u001b[0;32m    199\u001b[0m     huggingface_hub\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mEntryNotFoundError,\n\u001b[0;32m    200\u001b[0m     huggingface_hub\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mRevisionNotFoundError,\n\u001b[0;32m    201\u001b[0m     huggingface_hub\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mGatedRepoError,\n\u001b[0;32m    202\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\hf_api.py:5475\u001b[0m, in \u001b[0;36mHfApi.hf_hub_download\u001b[1;34m(self, repo_id, filename, subfolder, repo_type, revision, cache_dir, local_dir, force_download, proxies, etag_timeout, token, local_files_only, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m   5471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5472\u001b[0m     \u001b[38;5;66;03m# Cannot do `token = token or self.token` as token can be `False`.\u001b[39;00m\n\u001b[0;32m   5473\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken\n\u001b[1;32m-> 5475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hf_hub_download(\n\u001b[0;32m   5476\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[0;32m   5477\u001b[0m     filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   5478\u001b[0m     subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[0;32m   5479\u001b[0m     repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m   5480\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   5481\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint,\n\u001b[0;32m   5482\u001b[0m     library_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlibrary_name,\n\u001b[0;32m   5483\u001b[0m     library_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlibrary_version,\n\u001b[0;32m   5484\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   5485\u001b[0m     local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   5486\u001b[0m     local_dir_use_symlinks\u001b[38;5;241m=\u001b[39mlocal_dir_use_symlinks,\n\u001b[0;32m   5487\u001b[0m     user_agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_agent,\n\u001b[0;32m   5488\u001b[0m     force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   5489\u001b[0m     force_filename\u001b[38;5;241m=\u001b[39mforce_filename,\n\u001b[0;32m   5490\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   5491\u001b[0m     etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[0;32m   5492\u001b[0m     resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m   5493\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   5494\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   5495\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   5496\u001b[0m )\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:961\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m    942\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m    943\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    958\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    959\u001b[0m     )\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[0;32m    962\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m    963\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[0;32m    965\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[0;32m    966\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m    967\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m    968\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    969\u001b[0m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[0;32m    970\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[0;32m    971\u001b[0m         etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[0;32m    972\u001b[0m         headers\u001b[38;5;241m=\u001b[39mhf_headers,\n\u001b[0;32m    973\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    974\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    975\u001b[0m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[0;32m    976\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    977\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    978\u001b[0m     )\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:1112\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1110\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[1;32m-> 1112\u001b[0m     _download_to_tmp_and_move(\n\u001b[0;32m   1113\u001b[0m         incomplete_path\u001b[38;5;241m=\u001b[39mPath(blob_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.incomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1114\u001b[0m         destination_path\u001b[38;5;241m=\u001b[39mPath(blob_path),\n\u001b[0;32m   1115\u001b[0m         url_to_download\u001b[38;5;241m=\u001b[39murl_to_download,\n\u001b[0;32m   1116\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1117\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1118\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m   1119\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   1120\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1121\u001b[0m         etag\u001b[38;5;241m=\u001b[39metag,\n\u001b[0;32m   1122\u001b[0m         xet_file_data\u001b[38;5;241m=\u001b[39mxet_file_data,\n\u001b[0;32m   1123\u001b[0m     )\n\u001b[0;32m   1124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[0;32m   1125\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:1675\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[1;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[0m\n\u001b[0;32m   1669\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1670\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m   1671\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXet Storage is enabled for this repo, but the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_xet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m package is not installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1672\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalling back to regular HTTP download. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1673\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1674\u001b[0m             )\n\u001b[1;32m-> 1675\u001b[0m         http_get(\n\u001b[0;32m   1676\u001b[0m             url_to_download,\n\u001b[0;32m   1677\u001b[0m             f,\n\u001b[0;32m   1678\u001b[0m             proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1679\u001b[0m             resume_size\u001b[38;5;241m=\u001b[39mresume_size,\n\u001b[0;32m   1680\u001b[0m             headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1681\u001b[0m             expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m   1682\u001b[0m         )\n\u001b[0;32m   1684\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1685\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:452\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    451\u001b[0m     progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n\u001b[1;32m--> 452\u001b[0m     temp_file\u001b[38;5;241m.\u001b[39mwrite(chunk)\n\u001b[0;32m    453\u001b[0m     new_resume_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;66;03m# Some data has been downloaded from the server so we reset the number of retries.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_path = Path(\"F:/BigDataA3/data_compressed\")\n",
    "base_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "download_all_amazon_reviews(\n",
    "    base_save_path=str(base_path),\n",
    "    categories=None,  # All categories\n",
    "    compress=True,\n",
    "    compression_format=\"gz\",\n",
    "    compression_level=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39e9d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(base_path.glob(\"*.tar.gz\"))\n",
    "print(f\"Total compressed files downloaded: {len(files)}\")\n",
    "for f in sorted(files):\n",
    "    print(f.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b491f4",
   "metadata": {},
   "source": [
    "### Sample Review/Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c45b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sample = load_compressed_dataset(base_path / \"raw_review_Books.tar.gz\")\n",
    "meta_sample = load_compressed_dataset(base_path / \"raw_meta_Books.tar.gz\")\n",
    "\n",
    "print(\"Sample Review:\")\n",
    "print(review_sample[0])\n",
    "\n",
    "print(\"\\nSample Metadata:\")\n",
    "print(meta_sample[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e04c6b1",
   "metadata": {},
   "source": [
    "### Download log to view downloaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59492cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"download_log.txt\", \"w\") as f:\n",
    "    for file in sorted(files):\n",
    "        f.write(str(file.name) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
